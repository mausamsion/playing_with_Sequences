{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disecting LSTMs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mausamsion/playing_with_Sequences/blob/master/Disecting_LSTMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OM3UPvfTogF",
        "colab_type": "text"
      },
      "source": [
        "Reference:\n",
        "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
        "\n",
        "Actual paper:\n",
        "https://arxiv.org/abs/1409.0473"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R30jeo4vT8gP",
        "colab_type": "text"
      },
      "source": [
        "# What the heck is LSTM/GRU actually doing !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaCuP_-T2Eb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de\n",
        "!pip install --upgrade torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpy29Nm32Ixq",
        "colab_type": "code",
        "outputId": "089a5b5d-0314-4302-8385-e280796a53d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 810kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 244kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 237kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUzIgR6M2Nqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPzYPe2z33aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 4\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPvyc7To4CI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from typing import Tuple\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oqi2vj64SIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.dropout = dropout\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tuple[Tensor]:\n",
        "        print(\"\\n\\t--- ENCODER ---\\n\")\n",
        "        print(\"Parameters-\")\n",
        "        print(\"src: {}\".format(src.shape))\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        print(\"embedded:\", embedded.shape)\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "\n",
        "        print(\"\\noutput values:\", outputs)\n",
        "        print(\"\\nhidden values:\", torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        \n",
        "        print(\"outputs:\", outputs.shape)\n",
        "        print(\"hidden:\", hidden.shape)\n",
        "        \n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        print(\"\\nhidden mod values:\", hidden)\n",
        "        print(\"hidden mod:\", hidden.shape)\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYOo8vhy4aat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super().__init__()\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tensor:\n",
        "        print(\"\\n\\t--- ATTENTION ---\\n\")\n",
        "        print(\"Parameters-\")\n",
        "        print(\"decoder_hidden: {}, encoder_outputs: {}\".format(decoder_hidden.shape, encoder_outputs.shape))\n",
        "        \n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        print(\"src_len:\", src_len)\n",
        "\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "\n",
        "        print(\"repeated_decoder_hidden:\", repeated_decoder_hidden.shape)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "\n",
        "        print(\"encoder_outputs:\", encoder_outputs.shape)\n",
        "\n",
        "        energy = torch.tanh(self.attn(torch.cat((repeated_decoder_hidden, encoder_outputs), dim = 2)))\n",
        "\n",
        "        print(\"energy:\", energy.shape)\n",
        "\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        print(\"attention:\", attention.shape)\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOfxGQRx4aZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: int,\n",
        "                 attention: nn.Module):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _weighted_encoder_rep(self,\n",
        "                              decoder_hidden: Tensor,\n",
        "                              encoder_outputs: Tensor) -> Tensor:\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "        \n",
        "        print(\"  a:\", a.shape)\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        print(\"  a unsqueezed:\", a.shape)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        print(\"  encoder_outputs:\", encoder_outputs.shape)\n",
        "        \n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        print(\"  weighted_encoder_rep:\", weighted_encoder_rep.shape)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "\n",
        "        print(\"  weighted_encoder_rep:\", weighted_encoder_rep.shape)\n",
        "\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "    def forward(self,\n",
        "                input: Tensor,\n",
        "                decoder_hidden: Tensor,\n",
        "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
        "        print(\"\\n\\t--- DECODER ---\\n\")\n",
        "        print(\"Parameters-\")\n",
        "        print(\"input: {}, decoder_hidden: {}, encoder_outputs: {}\".format(input.shape, decoder_hidden.shape, encoder_outputs.shape))\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        print(\"input:\", input.shape)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "        print(\"embedded:\", embedded.shape)\n",
        "\n",
        "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        print(\"weighted_encoder_rep:\", weighted_encoder_rep.shape)\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
        "\n",
        "        print(\"rnn_input:\", rnn_input.shape)\n",
        "\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        print(\"output:\", output.shape, \", decoder_hidden:\", decoder_hidden.shape)\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "\n",
        "        print(\"embedded squeezed:\", embedded.shape)\n",
        "\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        print(\"output squeezed:\", output.shape)\n",
        "\n",
        "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
        "\n",
        "        print(\"weighted_encoder_rep squeezed:\", weighted_encoder_rep.shape)\n",
        "\n",
        "        output = self.out(torch.cat((output,\n",
        "                                     weighted_encoder_rep,\n",
        "                                     embedded), dim = 1))\n",
        "        \n",
        "        print(\"output (final):\", output.shape)\n",
        "\n",
        "        return output, decoder_hidden.squeeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW2B-Nir4aWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
        "        print(\"\\n\\t--- SEQ2SEQ ---\\n\")\n",
        "        print(\"Parameters-\")\n",
        "        print(\"src: {}, trg: {}\".format(src.shape, trg.shape))\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        print(\"outputs:\", outputs.shape)\n",
        "        \n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        \n",
        "        print(\"encoder_outputs:\", encoder_outputs.shape)\n",
        "        print(\"hidden:\", hidden.shape)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        output = trg[0,:]\n",
        "        \n",
        "        print(\"output:\", output.shape)\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if teacher_force else top1)\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbMvdvJT4aTd",
        "colab_type": "code",
        "outputId": "8d40e71f-cb31-4c9d-c626-0fce69749927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "# ENC_EMB_DIM = 256\n",
        "# DEC_EMB_DIM = 256\n",
        "# ENC_HID_DIM = 512\n",
        "# DEC_HID_DIM = 512\n",
        "# ATTN_DIM = 64\n",
        "# ENC_DROPOUT = 0.5\n",
        "# DEC_DROPOUT = 0.5\n",
        "\n",
        "# ENC_EMB_DIM = 32\n",
        "# DEC_EMB_DIM = 32\n",
        "# ENC_HID_DIM = 64\n",
        "# DEC_HID_DIM = 64\n",
        "# ATTN_DIM = 8\n",
        "\n",
        "ENC_EMB_DIM = 2\n",
        "DEC_EMB_DIM = 2\n",
        "ENC_HID_DIM = 4\n",
        "DEC_HID_DIM = 4\n",
        "ATTN_DIM = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "def count_parameters(model: nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 116,337 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbIpmkZf5LZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXEbYfq05LWg",
        "colab_type": "code",
        "outputId": "cb088b81-67e6-4bad-b0c8-cb9787a30b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for _, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        break\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "N_EPOCHS = 1\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    # valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "# test_loss = evaluate(model, test_iterator, criterion)\n",
        "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t--- SEQ2SEQ ---\n",
            "\n",
            "Parameters-\n",
            "src: torch.Size([14, 4]), trg: torch.Size([18, 4])\n",
            "outputs: torch.Size([18, 4, 5893])\n",
            "\n",
            "\t--- ENCODER ---\n",
            "\n",
            "Parameters-\n",
            "src: torch.Size([14, 4])\n",
            "embedded: torch.Size([14, 4, 2])\n",
            "\n",
            "output values: tensor([[[-1.5934e-05, -9.0265e-07, -3.6109e-06,  2.2339e-05, -6.5083e-06,\n",
            "           1.9935e-05,  2.3175e-05, -1.6225e-06],\n",
            "         [-9.4182e-06, -2.9124e-06, -1.0047e-05,  3.1819e-05, -2.9814e-05,\n",
            "          -1.5649e-06,  4.5519e-05,  4.3488e-06],\n",
            "         [-1.5934e-05, -9.0265e-07, -3.6109e-06,  2.2339e-05, -3.8113e-06,\n",
            "           9.8370e-06,  1.2330e-05, -1.2289e-06],\n",
            "         [ 6.5154e-06, -2.0097e-06, -6.4361e-06,  9.4794e-06, -1.1918e-05,\n",
            "          -1.5920e-05,  8.1693e-06,  3.7198e-06]],\n",
            "\n",
            "        [[-8.0484e-06, -5.7864e-07, -1.7834e-06,  1.1132e-05, -6.8514e-06,\n",
            "           1.7699e-05,  2.2251e-05, -9.4991e-07],\n",
            "         [-1.4739e-05, -2.1741e-06, -7.2107e-06,  2.9758e-05, -3.9805e-05,\n",
            "          -2.6696e-06,  6.0403e-05,  5.6155e-06],\n",
            "         [-8.0484e-06, -5.7864e-07, -1.7834e-06,  1.1132e-05, -1.5538e-06,\n",
            "          -2.5658e-06,  6.0545e-07,  1.2212e-07],\n",
            "         [-1.8745e-05,  5.7491e-06,  1.8544e-05, -2.7290e-05, -1.0496e-05,\n",
            "          -9.4863e-06,  1.0051e-05,  2.2340e-06]],\n",
            "\n",
            "        [[ 4.3589e-05, -1.9023e-05, -6.1156e-05,  1.0594e-04, -1.3483e-05,\n",
            "           3.5575e-05,  4.4384e-05, -2.4448e-06],\n",
            "         [-1.0664e-04, -6.8622e-06, -2.6031e-05,  1.5383e-04, -7.4763e-05,\n",
            "          -1.8286e-05,  1.0501e-04,  1.2077e-05],\n",
            "         [-9.2155e-05,  2.6814e-05,  8.6137e-05, -1.2260e-04, -3.0759e-06,\n",
            "          -5.0924e-06,  1.1778e-06,  2.6570e-07],\n",
            "         [-5.5777e-05,  1.2286e-05,  3.8417e-05, -4.2015e-05, -6.4625e-05,\n",
            "          -9.3198e-05,  4.0157e-05,  2.2076e-05]],\n",
            "\n",
            "        [[-1.6343e-04,  1.6296e-05,  4.9817e-05,  2.5114e-05,  9.4460e-05,\n",
            "           2.5446e-04,  1.8152e-05, -5.1044e-05],\n",
            "         [-4.9147e-05, -5.7605e-06, -1.7527e-05,  8.3527e-05, -1.1034e-04,\n",
            "          -1.7377e-04,  5.9111e-05,  4.0543e-05],\n",
            "         [ 9.9191e-05, -3.1017e-05, -1.0081e-04,  1.5040e-04, -1.8197e-04,\n",
            "          -3.0855e-04,  8.3541e-05,  7.1357e-05],\n",
            "         [ 1.4298e-04, -4.6503e-05, -1.4981e-04,  2.2787e-04, -1.8780e-04,\n",
            "          -3.1505e-04,  8.8427e-05,  7.3239e-05]],\n",
            "\n",
            "        [[-4.7390e-05, -2.9642e-06, -9.6305e-06,  6.3175e-05,  1.7495e-05,\n",
            "           4.8102e-05,  4.0094e-06, -8.9796e-06],\n",
            "         [ 1.0842e-04, -4.4457e-05, -1.4031e-04,  2.3555e-04, -2.0820e-04,\n",
            "          -3.2892e-04,  1.1125e-04,  7.8067e-05],\n",
            "         [ 9.7065e-05, -3.7201e-05, -1.1858e-04,  1.9502e-04, -6.8981e-05,\n",
            "          -1.2137e-04,  2.8780e-05,  2.7820e-05],\n",
            "         [ 7.1108e-05, -2.3948e-05, -7.3930e-05,  1.1297e-04, -2.8973e-05,\n",
            "          -4.7040e-05,  1.4624e-05,  1.0868e-05]],\n",
            "\n",
            "        [[-1.3751e-04,  3.3181e-05,  1.0745e-04, -1.3375e-04,  1.0372e-04,\n",
            "           2.1309e-04, -2.3496e-05, -4.6339e-05],\n",
            "         [ 1.7662e-04, -6.5338e-05, -2.0528e-04,  3.3013e-04, -1.4433e-04,\n",
            "          -2.0148e-04,  9.4532e-05,  5.0487e-05],\n",
            "         [ 1.4556e-04, -1.4398e-05, -3.8652e-05, -3.4662e-05,  1.6363e-06,\n",
            "          -4.4073e-05, -3.1532e-05,  6.9030e-06],\n",
            "         [ 9.0976e-05, -2.9469e-05, -9.1411e-05,  1.3691e-04, -5.7148e-05,\n",
            "          -9.3398e-05,  2.8560e-05,  2.1969e-05]],\n",
            "\n",
            "        [[-1.4826e-04,  1.2377e-05,  3.4981e-05,  4.5435e-05, -2.2212e-05,\n",
            "           3.9026e-05,  6.0319e-05, -3.1504e-06],\n",
            "         [ 2.1095e-05, -3.7505e-05, -1.1640e-04,  2.5713e-04, -1.0496e-05,\n",
            "           4.0504e-05,  4.2936e-05, -4.8177e-06],\n",
            "         [-6.5743e-06, -1.1272e-05, -3.7114e-05,  9.4148e-05, -2.8885e-05,\n",
            "           5.1988e-05,  7.9184e-05, -3.9526e-06],\n",
            "         [ 4.5269e-05, -1.5141e-05, -4.5107e-05,  6.7862e-05, -1.6303e-06,\n",
            "           2.8810e-06,  4.4382e-06, -2.8005e-07]],\n",
            "\n",
            "        [[-7.4466e-05,  5.6995e-06,  1.7253e-05,  2.2860e-05, -1.3975e-05,\n",
            "          -3.3051e-05,  3.2117e-07,  6.7079e-06],\n",
            "         [ 2.7875e-05, -1.8794e-05, -5.3373e-05,  1.0255e-04,  4.2498e-06,\n",
            "          -1.2106e-05, -1.4570e-05,  1.2850e-06],\n",
            "         [-3.9319e-05, -8.0779e-06, -2.6426e-05,  9.6963e-05, -2.7121e-05,\n",
            "          -6.9551e-06,  3.7924e-05,  4.6668e-06],\n",
            "         [-7.3034e-06, -9.4594e-06, -2.9017e-05,  7.5457e-05, -3.2131e-06,\n",
            "           5.8051e-06,  8.8476e-06, -6.5948e-07]],\n",
            "\n",
            "        [[-2.7716e-06, -8.0796e-06, -2.5699e-05,  6.1886e-05, -2.7573e-05,\n",
            "          -6.5782e-05,  2.9279e-07,  1.3741e-05],\n",
            "         [ 1.3719e-05, -9.7785e-06, -2.6334e-05,  5.0897e-05,  1.6547e-06,\n",
            "           8.5614e-07, -2.0277e-06, -3.1351e-07],\n",
            "         [ 1.4663e-05, -1.5214e-05, -4.7254e-05,  9.8646e-05, -4.0092e-05,\n",
            "          -6.3412e-05,  2.1395e-05,  1.5245e-05],\n",
            "         [ 2.5538e-05, -3.4013e-06, -7.6572e-06, -3.7089e-06,  4.8231e-06,\n",
            "          -3.0132e-05, -2.7275e-05,  3.8631e-06]],\n",
            "\n",
            "        [[ 5.9394e-05, -8.6031e-07,  1.1303e-06, -5.4711e-05,  1.4786e-05,\n",
            "          -1.3632e-05, -3.2070e-05,  3.0172e-07],\n",
            "         [-4.1203e-06, -5.6939e-06, -1.5456e-05,  4.0505e-05,  3.2633e-06,\n",
            "           1.6717e-06, -4.0194e-06, -6.0332e-07],\n",
            "         [ 1.7841e-05, -1.1314e-05, -3.3930e-05,  6.4619e-05, -9.8916e-06,\n",
            "          -8.5834e-06,  9.8764e-06,  2.9425e-06],\n",
            "         [ 4.5366e-05,  2.1055e-07,  3.6018e-06, -4.7533e-05, -1.5048e-06,\n",
            "          -1.9187e-05, -1.0233e-05,  3.1651e-06]],\n",
            "\n",
            "        [[-3.3878e-05, -3.6930e-06, -1.3894e-05,  6.2131e-05,  6.3535e-06,\n",
            "           5.7828e-05,  2.7942e-05, -9.7362e-06],\n",
            "         [ 5.8775e-05,  4.2455e-07,  6.1886e-06, -6.5335e-05,  1.0502e-05,\n",
            "          -1.1983e-05, -2.4340e-05,  8.0119e-07],\n",
            "         [ 8.7815e-06, -5.8979e-06, -1.6743e-05,  3.2075e-05,  1.9558e-06,\n",
            "           1.9452e-05,  9.6162e-06, -2.8245e-06],\n",
            "         [-5.5034e-06, -2.5442e-06, -9.0922e-06,  2.6526e-05, -1.5149e-05,\n",
            "           7.2917e-06,  2.8489e-05,  8.1561e-07]],\n",
            "\n",
            "        [[-1.7150e-05, -2.1800e-06, -6.8604e-06,  3.0941e-05,  3.6429e-05,\n",
            "           2.6085e-05, -3.9935e-05, -8.9906e-06],\n",
            "         [-3.4160e-05, -3.0050e-06, -1.1396e-05,  5.6852e-05, -2.0891e-06,\n",
            "           6.1233e-05,  4.3317e-05, -8.8807e-06],\n",
            "         [ 4.3226e-06, -3.0683e-06, -8.2609e-06,  1.5920e-05,  3.8735e-06,\n",
            "           3.8867e-05,  1.9305e-05, -6.0024e-06],\n",
            "         [-2.8290e-06, -1.3964e-06, -4.4880e-06,  1.3194e-05, -1.0571e-05,\n",
            "          -1.7561e-05,  5.0580e-06,  3.9377e-06]],\n",
            "\n",
            "        [[-1.7873e-06,  1.3205e-05,  4.4991e-05, -1.0439e-04,  7.1830e-05,\n",
            "           5.1290e-05, -7.9059e-05, -1.7595e-05],\n",
            "         [-1.7278e-05, -1.8134e-06, -5.6270e-06,  2.8318e-05,  1.9788e-05,\n",
            "           3.3102e-05, -9.3608e-06, -7.5642e-06],\n",
            "         [-6.1676e-05, -5.2076e-06, -1.8533e-05,  9.7341e-05,  7.6667e-06,\n",
            "           7.7666e-05,  3.8756e-05, -1.2707e-05],\n",
            "         [-1.4527e-06, -7.6006e-07, -2.2150e-06,  6.5620e-06, -2.0853e-05,\n",
            "          -3.4874e-05,  9.8647e-06,  7.9701e-06]],\n",
            "\n",
            "        [[ 6.2683e-06,  2.1509e-05,  7.0582e-05, -1.7167e-04,  4.7657e-05,\n",
            "           3.4000e-05, -5.2527e-05, -1.1644e-05],\n",
            "         [-4.7283e-05,  1.0827e-05,  3.5300e-05, -4.1974e-05,  3.9032e-05,\n",
            "           6.5740e-05, -1.8250e-05, -1.5308e-05],\n",
            "         [-6.9726e-05,  8.7459e-06,  2.8927e-05, -7.5885e-06,  3.9032e-05,\n",
            "           6.5740e-05, -1.8250e-05, -1.5308e-05],\n",
            "         [ 3.9866e-05, -1.2938e-05, -4.1210e-05,  6.2353e-05, -4.1133e-05,\n",
            "          -6.9259e-05,  1.9232e-05,  1.6129e-05]]], grad_fn=<CatBackward>)\n",
            "\n",
            "hidden values: tensor([[ 6.2683e-06,  2.1509e-05,  7.0582e-05, -1.7167e-04, -6.5083e-06,\n",
            "          1.9935e-05,  2.3175e-05, -1.6225e-06],\n",
            "        [-4.7283e-05,  1.0827e-05,  3.5300e-05, -4.1974e-05, -2.9814e-05,\n",
            "         -1.5649e-06,  4.5519e-05,  4.3488e-06],\n",
            "        [-6.9726e-05,  8.7459e-06,  2.8927e-05, -7.5885e-06, -3.8113e-06,\n",
            "          9.8370e-06,  1.2330e-05, -1.2289e-06],\n",
            "        [ 3.9866e-05, -1.2938e-05, -4.1210e-05,  6.2353e-05, -1.1918e-05,\n",
            "         -1.5920e-05,  8.1693e-06,  3.7198e-06]], grad_fn=<CatBackward>)\n",
            "outputs: torch.Size([14, 4, 8])\n",
            "hidden: torch.Size([2, 4, 4])\n",
            "\n",
            "hidden mod values: tensor([[-8.8775e-08,  5.3182e-07,  8.1773e-07, -1.2034e-06],\n",
            "        [-1.1028e-08,  1.7256e-06,  1.3258e-06, -1.8613e-06],\n",
            "        [ 2.9264e-07,  1.6706e-06,  1.2150e-06, -8.7142e-07],\n",
            "        [-1.2091e-07, -8.9038e-07, -6.8915e-07,  2.6407e-07]],\n",
            "       grad_fn=<TanhBackward>)\n",
            "hidden mod: torch.Size([4, 4])\n",
            "encoder_outputs: torch.Size([14, 4, 8])\n",
            "hidden: torch.Size([4, 4])\n",
            "output: torch.Size([4])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "\n",
            "\t--- DECODER ---\n",
            "\n",
            "Parameters-\n",
            "input: torch.Size([4]), decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "input: torch.Size([1, 4])\n",
            "embedded: torch.Size([1, 4, 2])\n",
            "\n",
            "\t--- ATTENTION ---\n",
            "\n",
            "Parameters-\n",
            "decoder_hidden: torch.Size([4, 4]), encoder_outputs: torch.Size([14, 4, 8])\n",
            "src_len: 14\n",
            "repeated_decoder_hidden: torch.Size([4, 14, 4])\n",
            "encoder_outputs: torch.Size([4, 14, 8])\n",
            "energy: torch.Size([4, 14, 2])\n",
            "attention: torch.Size([4, 14])\n",
            "  a: torch.Size([4, 14])\n",
            "  a unsqueezed: torch.Size([4, 1, 14])\n",
            "  encoder_outputs: torch.Size([4, 14, 8])\n",
            "  weighted_encoder_rep: torch.Size([4, 1, 8])\n",
            "  weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "weighted_encoder_rep: torch.Size([1, 4, 8])\n",
            "rnn_input: torch.Size([1, 4, 10])\n",
            "output: torch.Size([1, 4, 4]) , decoder_hidden: torch.Size([1, 4, 4])\n",
            "embedded squeezed: torch.Size([4, 2])\n",
            "output squeezed: torch.Size([4, 4])\n",
            "weighted_encoder_rep squeezed: torch.Size([4, 8])\n",
            "output (final): torch.Size([4, 5893])\n",
            "Epoch: 01 | Time: 0m 0s\n",
            "\tTrain Loss: 0.001 | Train PPL:   1.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em5y3fLj5LTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG6L_7Up4aHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whBeLLZs4aE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hJnqqQC4aB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4f_SSsn4Z_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGvFciOD4Z0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
